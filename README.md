# GraphQL Benchmarking Toolset

## Introduction

This repo contains four top-level folders, and a Makefile. This is the significance of each:

- `/containers`: Has the `docker-compose.yaml` manifest for Hasura + Postgres 12. By default, Hasura is bound to `localhost:8085` not to conflict, and postgres to `localhost:5430`. Feel free to modify these if you like. It also contains a SQL script for creating the Chinook database, and a bash script for loading it into Postgres as-configured.

- `/queries`: This folder contains the code for benchmarking GraphQL queries. Two different benchmarking tools are used to get a better estimate. Autocannon, written in Node, and K6, written in Go (with a JS scripting API).

- `/subscriptions`: Contains the code for benchmarking subscriptions (this contains it's own README and set of instructions, please see it for details).

- `/reports`: The output of both JSON and HTML reports when running benchmarks goes here. It contains two folders, `/k6` and `/autocannon` which each hold their respective report data.

There is a makefile in the root of the project, with a help command available. Running `make` without arguments or `make help` will prompt it:

```
❯ make
00README                       RECOMMENDED PROCESS: "make setup_containers" -> "make create_chinook_database" -> "make benchmark_all_then_serve_reports"
benchmark_all_then_serve_reports Runs benchmarks with Autocannon & K6, then serves their reports on http://localhost:5000
benchmark_autocannon           Runs benchmarks using Autocannon, outputs HTML & JSON reports per-query to './reports/autocannon'
benchmark_k6                   Runs benchmarks using K6, outputs JSON reports per-query to './reports/k6'
clean_reports                  Remove previously generated bench reports
create_chinook_database        Sets up Chinook database in Hasura for testing
do_everything_for_me           Sets up containers, creates Chinook database, runs benchmarks with Autocannon & K6, then serves reports
install_node_modules           Installs nodes modules for query dependencies
serve_reports                  Starts a webserver to display the output of Autocannon and K6 reports on http://localhost:5000
setup_all                      Sets up containers and then creates Chinook database
setup_containers               Sets up Hasura and Postgres Docker containers
```

The easiest path, as the recommendation states, is likely to run `make setup_container`, `make_create_chinook_database`, and then `make benchmark_all_then_serve` to see if any issues arise at one of those points. You can also run `make do_everything_for_me` and cross your fingers though.

## Requirements

- Node
- Docker
- Docker Compose
- Unix-based OS or Dockerize the setup/run in VM

## Queries

### Configuration

The query configuration is done in a file `./queries/config.yaml`. Below is an explanation of each setting:

```yaml
# URL to Hasura endpoint
url: http://localhost:8085/v1/graphql
# (Optional) Headers object
headers:
  X-Hasura-Admin-Secret: my-secret
# Array of query objects
queries:
  # Name identifies it on the output reports
  - name: SearchAlbumsWithArtist
    # Duration is a timestring
    duration: 10s
    # Requests per second
    rps: 400
    # (Optional) Assert that the number of results returned by query matches a number
    assert:
      results: 7
    # Query string
    query: |
      query SearchAlbumsWithArtist {
        albums(where: {title: {_like: "%Rock%"}}) {
          id
          title
          artist {
            name
            id
          }
        }
      }
  - name: AlbumByPK
    duration: 10s
    rps: 400
    assert:
      results: 1
    query: |
      query AlbumByPK($id: Int!) {
        albums_by_pk(id: $id) {
          id
          title
        }
      }
    # (Optional) Variables object, easier just to send direct queries
    variables:
      id: 5
```

### Running

You can call any of the following to run benchmarks:

- `make benchmark_k6`
- `make benchmark_autocannon`
- `make benchmark_all_then_serve_reports`

If you want to run the entire process of benchmarking manually, you can do:

- `make benchmark_k6 benchmark_autocannon serve_reports`

After running this process, during the benchmarking, you should have gotten terminal stdout output like this:

| Autocannon                               | K6                               |
| ---------------------------------------- | -------------------------------- |
| ![](readme_images/autocannon-output.png) | ![](readme_images/k6-output.png) |

And if you chose to run `make benchmark_all_then_serve_reports`, then finally it should be serving the finished reports on `http://localhost:5000`:

| Output                                  | Web index                          |
| --------------------------------------- | ---------------------------------- |
| ![](readme_images/npx-serve-output.png) | ![](readme_images/serve-index.png) |

Inside of the `autocannon` directory, you should see several HTML files with query names, one per query, with reports like below:

![](readme_images/autocannon-report.png)

Clicking on `k6` will take you to the generated plot from the aggregate query data:

![](readme_images/k6s-report.png)


## Run once as docker container

To run the benchmarking suite once as a docker image, you can build and run the image as follows.

```sh
cd app && docker-compose up --build --force-recreate && docker run \
-e GIT_TOKEN=<CHANGEMEtoken> \
-e GIT_EMAIL=user@example.com \
-e GIT_REMOTE=https://<user>:<tokenCHANGEME>@github.com/<user>/<repo_name>.git \
-e GIT_REPO_NAME=auto-test \
-e GIT_REPORTS_DIR=stage12 \
-e GIT_NAME=soorajshankar \
-it app_graphql-bench query \
--config https://gist.githubusercontent.com/<CHANGE MEyour gitst url>/config.yaml #test configuration file url \
-o reports.json
```

1. Replace the configuration with the git configurataion, this is responsible of pubishing the reports on to the git repository, (any invalid configuration will fail the auto-publishing and as a fallback mechanism, reports will be printed on the container console output)

2. the test configuration file url provided will be used to source the test suites, and to define the benchmarking configuration  